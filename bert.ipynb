{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303eaf90-cfc3-4021-b0b3-e5acb44a9c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: 124 ['accident and emergency incident', 'accomplishment', 'act of terror', 'animal', 'anniversary', 'armed conflict', 'arts and entertainment', 'arts, culture, entertainment and media', 'biomedical science', 'bodybuilding']\n",
      "<_TakeDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(124,), dtype=tf.int32, name=None))> <_TakeDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(124,), dtype=tf.int32, name=None))> <_SkipDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(124,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def remove_character_from_column(\n",
    "    df: pd.DataFrame,\n",
    "    columname: str,\n",
    "    character_to_remove: str,\n",
    "    new_character: str = \"\",\n",
    "    is_regex: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace occurrences of pattern/character in a DataFrame column.\n",
    "    \"\"\"\n",
    "    if columname not in df.columns:\n",
    "        raise ValueError(f\"Column '{columname}' not found in DataFrame. Available: {list(df.columns)}\")\n",
    "\n",
    "    df = df.copy()\n",
    "   \n",
    "    s = df[columname].fillna(\"\").astype(str)\n",
    "\n",
    "    if is_regex:\n",
    "        df[columname] = s.str.replace(character_to_remove, new_character, regex=True)\n",
    "    else:\n",
    "        df[columname] = s.str.replace(character_to_remove, new_character, regex=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_characters(df_to_replace, columname='text', character_patterns=None, is_regex=True):\n",
    "    \"\"\"\n",
    "    La tua funzione, identica, ma in versione standalone.\n",
    "    \"\"\"\n",
    "    if character_patterns is None:\n",
    "        character_patterns = [\n",
    "            (r'[\\u2000-\\u200B\\u3000\\xa0]', ' '),       # Unicode spaces\n",
    "            (r'[\\u200C\\u200D\\u2060\\uFEFF]', ''),       # Invisible characters\n",
    "            (r'[\\u201C\\u201D\\u2018\\u2019]', '\"'),      # Quotation marks\n",
    "            (r'[\\u2022\\u2043]', ''),                   # List symbols\n",
    "            (r'[\\U0001F600-\\U0001F64F]', ''),          # Emoji\n",
    "            (r'[\\u2026]', '...'),                      # Ellipsis\n",
    "            (r\"http\\S+|www.\\S+\", \"\"),                  # URL\n",
    "            (r\"[\\n\\t\\r]\", \"\")                          # Tabulation and newline characters\n",
    "        ]\n",
    "    else:\n",
    "        if not isinstance(character_patterns, list):\n",
    "            raise TypeError(\"character_patterns must be a list of tuples\")\n",
    "\n",
    "    for pattern, replacement in character_patterns:\n",
    "        df_to_replace = remove_character_from_column(\n",
    "            df_to_replace,\n",
    "            columname=columname,\n",
    "            character_to_remove=pattern,\n",
    "            new_character=replacement,\n",
    "            is_regex=is_regex\n",
    "        )\n",
    "\n",
    "    return df_to_replace\n",
    "\n",
    "\n",
    "def write_classes_to_file(classes, out_dir: str, filename: str = \"classes.txt\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    path = os.path.join(out_dir, filename)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for c in classes:\n",
    "            f.write(f\"{c}\\n\")\n",
    "    return path\n",
    "\n",
    "def read_classes_from_file(out_dir: str, filename: str = \"classes.txt\"):\n",
    "    path = os.path.join(out_dir, filename)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "\n",
    "def get_and_preprocess_mnds(\n",
    "    dim: int | None = None,\n",
    "    cache_dir: str = \"./data_mnds\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Simile al tuo:\n",
    "      - usa caching su JSONL\n",
    "      - usa ML Binarizer su category_level_1 + category_level_2\n",
    "      - pulisce il testo con replace_characters (la tua)\n",
    "      - restituisce tf.data.Dataset((text, multi_hot)) + classes_names\n",
    "    \"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    suffix = f\"_{dim}\" if dim is not None else \"\"\n",
    "    dataset_path = os.path.join(cache_dir, f\"mnds{suffix}.jsonl\")\n",
    "    raw_csv_path = os.path.join(cache_dir, f\"mnds{suffix}.csv\")\n",
    "    classes_path = os.path.join(cache_dir, \"classes.txt\")\n",
    "\n",
    "\n",
    "    if os.path.exists(dataset_path) and os.path.exists(classes_path):\n",
    "        df_final = pd.read_json(dataset_path, orient=\"records\", lines=True)\n",
    "        df_final_text = df_final[\"text\"].astype(str).values\n",
    "        df_final[\"category\"] = df_final[\"category\"].apply(lambda x: np.array(x, dtype=np.int32))\n",
    "        df_final_categories = np.array(df_final[\"category\"].tolist(), dtype=np.int32)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((df_final_text, df_final_categories))\n",
    "        classes_names = read_classes_from_file(cache_dir)\n",
    "        return dataset, classes_names\n",
    "\n",
    "\n",
    "    split = f\"train[:{dim}]\" if dim is not None else \"train\"\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "\n",
    "    if os.path.exists(raw_csv_path):\n",
    "        df = pd.read_csv(raw_csv_path)\n",
    "    else:\n",
    "        ds = load_dataset(\"textminr/mn-ds\", split=split)\n",
    "        ds.to_csv(raw_csv_path, index=False)\n",
    "        df = pd.DataFrame(ds)\n",
    "\n",
    "\n",
    "    if \"content\" in df.columns and \"text\" not in df.columns:\n",
    "        df.rename(columns={\"content\": \"text\"}, inplace=True)\n",
    "\n",
    "\n",
    "    needed = [\"text\", \"category_level_1\", \"category_level_2\"]\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonne mancanti: {missing}. Disponibili: {list(df.columns)}\")\n",
    "\n",
    "    df = df[needed].copy()\n",
    "    df[\"category\"] = None  \n",
    "\n",
    "\n",
    "    df_final = replace_characters(df, columname=\"text\")\n",
    "\n",
    "\n",
    "    category = (df_final[[\"category\", \"category_level_1\", \"category_level_2\"]].fillna(\"\").values.tolist())\n",
    "    category = [[el for el in row if str(el).strip() != \"\"] for row in category]\n",
    "\n",
    "    category_encoded = mlb.fit_transform(category).astype(np.int32)\n",
    "\n",
    "    df_final[\"category\"] = list(category_encoded)\n",
    "    df_final = df_final[[\"text\", \"category\"]].copy()\n",
    "\n",
    "    df_final_text = df_final[\"text\"].astype(str).values\n",
    "    df_final_categories = np.array(df_final[\"category\"].tolist(), dtype=np.int32)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((df_final_text, df_final_categories))\n",
    "\n",
    "\n",
    "    write_classes_to_file(mlb.classes_, cache_dir, filename=\"classes.txt\")\n",
    "    df_final.to_json(dataset_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "    classes_names = list(mlb.classes_)\n",
    "    return dataset, classes_names\n",
    "\n",
    "\n",
    "# Split \n",
    "\n",
    "def split_dataset(dataset, test_size: float = 0.1, val_size: float = 0.1, random_state: int = 42):\n",
    "    if not isinstance(dataset, (pd.DataFrame, tf.data.Dataset)):\n",
    "        raise TypeError(\"dataset must be either a Pandas DataFrame or a TensorFlow dataset\")\n",
    "    if not isinstance(test_size, float) or not isinstance(val_size, float):\n",
    "        raise TypeError(\"test_size and val_size must be floats\")\n",
    "    if not isinstance(random_state, int):\n",
    "        raise TypeError(\"random_state must be an integer\")\n",
    "    if test_size < 0.0 or test_size > 1.0:\n",
    "        raise ValueError(\"test_size must be between 0.0 and 1.0\")\n",
    "    if val_size < 0.0 or val_size > 1.0:\n",
    "        raise ValueError(\"val_size must be between 0.0 and 1.0\")\n",
    "    if random_state < 0:\n",
    "        raise ValueError(\"random_state must be a positive integer\")\n",
    "    if test_size + val_size >= 1.0:\n",
    "        raise ValueError(\"test_size + val_size must be < 1.0\")\n",
    "\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        return split_df_dataset(dataset, random_state, test_size, val_size)\n",
    "    else:\n",
    "        return split_tf_dataset(dataset, test_size, val_size, random_state)\n",
    "\n",
    "\n",
    "def split_df_dataset(dataset, random_state, test_size, val_size):\n",
    "    df_train_val, df_test = train_test_split(\n",
    "        dataset, test_size=test_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    val_relative = val_size / (1.0 - test_size)\n",
    "    df_train, df_val = train_test_split(\n",
    "        df_train_val, test_size=val_relative, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "def split_tf_dataset(dataset: tf.data.Dataset, test_size=0.1, val_size=0.1, random_seed=42, buffer_size=10000):\n",
    "    dataset = dataset.shuffle(buffer_size=buffer_size, seed=random_seed, reshuffle_each_iteration=False)\n",
    "\n",
    "    dataset_size = int(dataset.reduce(0, lambda x, _: x + 1).numpy())\n",
    "\n",
    "    test_n = int(test_size * dataset_size)\n",
    "    val_n = int(val_size * dataset_size)\n",
    "    train_n = dataset_size - test_n - val_n\n",
    "\n",
    "    train_dataset = dataset.take(train_n)\n",
    "    rest_dataset = dataset.skip(train_n)\n",
    "\n",
    "    val_dataset = rest_dataset.take(val_n)\n",
    "    test_dataset = rest_dataset.skip(val_n)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset, classes = get_and_preprocess_mnds(dim=2000, cache_dir=\"./data_mnds\")\n",
    "    print(\"labels:\", len(classes), classes[:10])\n",
    "\n",
    "    train_ds, val_ds, test_ds = split_dataset(dataset, test_size=0.1, val_size=0.1)\n",
    "    print(train_ds, val_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e216f652-8b42-4f45-bf39-f2957965dbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elimino: ./data_mnds\\mnds.jsonl\n",
      "Elimino: ./data_mnds\\mnds_2000.jsonl\n",
      "Elimino: ./data_mnds\\classes.txt\n",
      "Cache pulita.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "cache_dir = \"./data_mnds\"\n",
    "\n",
    "\n",
    "files = glob.glob(os.path.join(cache_dir, \"mnds*.jsonl\"))\n",
    "files += glob.glob(os.path.join(cache_dir, \"classes.txt\"))\n",
    "\n",
    "for f in files:\n",
    "    print(\"Elimino:\", f)\n",
    "    os.remove(f)\n",
    "\n",
    "print(\"Cache pulita.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e061414b-0690-4bdb-9d11-db7037781af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "CUDA available: True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "544de4eb-ddb3-4c3b-af4c-e665f3fb44de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cpu\n",
      "CUDA disponibile: False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA disponibile:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:582\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m device(cur_stream\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdst_prev_stream \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_stream(cur_stream\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 582\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_stream(cur_stream)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:614\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_stream_by_id\u001b[39m(stream_id, device_index, device_type):\n\u001b[0;32m    611\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"set stream specified by the stream id, device index and\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;124;03m        device type\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \n\u001b[1;32m--> 614\u001b[0m \u001b[38;5;124;03m    Args: stream_id (int): stream id in stream pool\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;124;03m          device_index (int): device index in topo\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;124;03m          device_type (int): enum device type\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_setStream(\n\u001b[0;32m    619\u001b[0m         stream_id\u001b[38;5;241m=\u001b[39mstream_id,\n\u001b[0;32m    620\u001b[0m         device_index\u001b[38;5;241m=\u001b[39mdevice_index,\n\u001b[0;32m    621\u001b[0m         device_type\u001b[38;5;241m=\u001b[39mdevice_type,\n\u001b[0;32m    622\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:403\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mcudaStatus\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m     SUCCESS: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    404\u001b[0m     ERROR_NOT_READY: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m34\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(\"CUDA disponibile:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8970f6-dd94-490c-8693-5cc5f02e3b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6667b1b8-00b5-4bd8-af1f-ddd60d93ac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.26.4\n",
      "torch: 2.5.1+cu121\n",
      "transformers: 4.57.3\n",
      "sklearn: 1.8.0\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import sklearn\n",
    "\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043d6465-e7d4-4f1b-9ecf-e0694d3b2593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2743fa2-59c8-4bbf-b546-ba957887cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pietr\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Torch: 2.5.1+cu121\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d28cc2b2-efa8-4294-84a5-af5256a5f3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_LABELS: 126\n",
      "sizes: 8735 1091 1091\n",
      "label shape: (8735, 126)\n"
     ]
    }
   ],
   "source": [
    "dataset, classes_names = get_and_preprocess_mnds(\n",
    "    dim=None,\n",
    "    cache_dir=\"./data_mnds\"\n",
    ")\n",
    "\n",
    "NUM_LABELS = len(classes_names)\n",
    "print(\"NUM_LABELS:\", NUM_LABELS)\n",
    "\n",
    "train_tf, val_tf, test_tf = split_dataset(\n",
    "    dataset,\n",
    "    test_size=0.1,\n",
    "    val_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "def tfds_to_numpy(ds, limit=None):\n",
    "    texts, labels = [], []\n",
    "    for i, (t, y) in enumerate(ds):\n",
    "        if limit is not None and i >= limit:\n",
    "            break\n",
    "        texts.append(t.numpy().decode(\"utf-8\"))\n",
    "        labels.append(y.numpy().astype(np.float32))\n",
    "    return texts, np.stack(labels)\n",
    "\n",
    "train_texts, train_y = tfds_to_numpy(train_tf)\n",
    "val_texts,   val_y   = tfds_to_numpy(val_tf)\n",
    "test_texts,  test_y  = tfds_to_numpy(test_tf)\n",
    "\n",
    "print(\"sizes:\", len(train_texts), len(val_texts), len(test_texts))\n",
    "print(\"label shape:\", train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd41da6c-d02f-4b1c-8a38-93a71b50f207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight stats: 0.18103567 1.9227242 2.4405632\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "pos = train_y.sum(axis=0)\n",
    "neg = train_y.shape[0] - pos\n",
    "\n",
    "pos_weight_base = (neg / (pos + 1e-8)).astype(np.float32)\n",
    "\n",
    "ALPHA = 0.02  # prova 0.02 se precision ancora bassa, 0.1 se recall troppo basso\n",
    "pos_weight = ALPHA * pos_weight_base\n",
    "\n",
    "pos_weight_t = torch.tensor(pos_weight, device=device)\n",
    "\n",
    "print(\"pos_weight stats:\",\n",
    "      pos_weight.min(),\n",
    "      pos_weight.mean(),\n",
    "      pos_weight.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bd1b8a5-7e48-4ec2-80b8-3308f7bc071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import Trainer\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, pos_weight=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "    \n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee5d3ecc-0771-40f7-8826-89c5756d2ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train (8735) ...\n",
      "Done train\n",
      "Tokenizing val (1091) ...\n",
      "Done val\n",
      "Tokenizing test (1091) ...\n",
      "Done test\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_all(texts, name):\n",
    "    print(f\"Tokenizing {name} ({len(texts)}) ...\")\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    print(f\"Done {name}\")\n",
    "    return enc\n",
    "\n",
    "train_enc = tokenize_all(train_texts, \"train\")\n",
    "val_enc   = tokenize_all(val_texts, \"val\")\n",
    "test_enc  = tokenize_all(test_texts, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ab4955c-e579-4190-a6d9-6486a0207699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodedMultiLabelDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            k: torch.tensor(v[idx], dtype=torch.long)\n",
    "            for k, v in self.encodings.items()\n",
    "        }\n",
    "        item[\"labels\"] = torch.tensor(\n",
    "            self.labels[idx], dtype=torch.float32\n",
    "        )\n",
    "        return item\n",
    "\n",
    "train_ds = EncodedMultiLabelDataset(train_enc, train_y)\n",
    "val_ds   = EncodedMultiLabelDataset(val_enc, val_y)\n",
    "test_ds  = EncodedMultiLabelDataset(test_enc, test_y)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4202072b-3155-4368-b3cc-970885d8edb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "089e03dc-b890-4638-8d9a-d67a6c08be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.105\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def macro_auc_per_label(y_true, y_score, kind=\"roc\"):\n",
    "    aucs = []\n",
    "    for j in range(y_true.shape[1]):\n",
    "        yt = y_true[:, j]\n",
    "        ys = y_score[:, j]\n",
    "        if np.unique(yt).size < 2:\n",
    "            continue\n",
    "        try:\n",
    "            if kind == \"roc\":\n",
    "                aucs.append(roc_auc_score(yt, ys))\n",
    "            else:\n",
    "                aucs.append(average_precision_score(yt, ys))\n",
    "        except Exception:\n",
    "            continue\n",
    "    return float(np.mean(aucs)) if aucs else float(\"nan\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = sigmoid(logits)\n",
    "\n",
    "    preds = (probs >= THRESHOLD).astype(int)\n",
    "    y_true = labels.astype(int)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(\n",
    "            y_true, preds, average=\"micro\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            y_true, preds, average=\"micro\", zero_division=0\n",
    "        ),\n",
    "        \"binary_accuracy\": accuracy_score(\n",
    "            y_true.reshape(-1),\n",
    "            preds.reshape(-1),\n",
    "        ),\n",
    "        \"auc_roc_macro\": macro_auc_per_label(\n",
    "            y_true, probs, kind=\"roc\"\n",
    "        ),\n",
    "        \"auc_pr_macro\": macro_auc_per_label(\n",
    "            y_true, probs, kind=\"pr\"\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1bacb9a-2a19-458f-86c9-a7b51a4b31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./bert_mnds_pt\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=5,               \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"auc_pr_macro\",  \n",
    "    greater_is_better=True,\n",
    "    fp16=(device.type == \"cuda\"),\n",
    "    dataloader_pin_memory=(device.type == \"cuda\"),\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    pos_weight=pos_weight_t,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4821827-f1d8-420b-8401-93d538d61c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a9f7d48-d65b-48eb-b6dd-488d82f4e543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5460' max='5460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5460/5460 13:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Binary Accuracy</th>\n",
       "      <th>Auc Roc Macro</th>\n",
       "      <th>Auc Pr Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.053940</td>\n",
       "      <td>0.383217</td>\n",
       "      <td>0.705316</td>\n",
       "      <td>0.977303</td>\n",
       "      <td>0.944286</td>\n",
       "      <td>0.578247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.049596</td>\n",
       "      <td>0.359019</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.974677</td>\n",
       "      <td>0.946246</td>\n",
       "      <td>0.600938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.047312</td>\n",
       "      <td>0.368351</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.975485</td>\n",
       "      <td>0.948695</td>\n",
       "      <td>0.603593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.046243</td>\n",
       "      <td>0.368917</td>\n",
       "      <td>0.771311</td>\n",
       "      <td>0.975427</td>\n",
       "      <td>0.949366</td>\n",
       "      <td>0.610811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.045942</td>\n",
       "      <td>0.371397</td>\n",
       "      <td>0.767644</td>\n",
       "      <td>0.975689</td>\n",
       "      <td>0.949047</td>\n",
       "      <td>0.611696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEST ---\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in ./bert_mnds_pt/best_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('disaster, accident and emergency incident', 0.7251623272895813),\n",
       "  ('emergency response', 0.5064884424209595),\n",
       "  ('disaster', 0.32777586579322815),\n",
       "  ('accident and emergency incident', 0.231440469622612),\n",
       "  ('emergency planning', 0.18624301254749298),\n",
       "  ('emergency incident', 0.1256006509065628)]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "print(\"\\n--- TEST ---\")\n",
    "trainer.evaluate(test_ds)\n",
    "\n",
    "SAVE_DIR = \"./bert_mnds_pt/best_model\"\n",
    "trainer.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(f\"Model saved in {SAVE_DIR}\")\n",
    "\n",
    "\n",
    "def predict_multilabel(texts, top_k=10):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(\n",
    "            model(**enc).logits\n",
    "        ).cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for p in probs:\n",
    "        idx_sorted = np.argsort(-p)[:top_k]\n",
    "        picked = [\n",
    "            (classes_names[i], float(p[i]))\n",
    "            for i in idx_sorted\n",
    "            if p[i] >= THRESHOLD\n",
    "        ]\n",
    "        results.append(picked)\n",
    "    return results\n",
    "\n",
    "\n",
    "predict_multilabel([\n",
    "    \"Breaking news: a major earthquake caused damage and emergency response teams were deployed.\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "195fd3b4-6760-4c00-b54e-eb45aeb9fa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.29545454545454547\n",
      "Precision: 0.6607862903225806\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "pred = trainer.predict(val_ds)\n",
    "probs = 1/(1+np.exp(-pred.predictions))\n",
    "y_true = pred.label_ids.astype(int)\n",
    "\n",
    "best_t, best_p = None, 0.0\n",
    "\n",
    "for t in np.linspace(0.05, 0.5, 100):\n",
    "    y_pred = (probs >= t).astype(int)\n",
    "    p = precision_score(y_true.reshape(-1), y_pred.reshape(-1), zero_division=0)\n",
    "    r = recall_score(y_true.reshape(-1), y_pred.reshape(-1), zero_division=0)\n",
    "\n",
    "    if r >= 0.6 and p > best_p:\n",
    "        best_p = p\n",
    "        best_t = t\n",
    "\n",
    "print(\"Best threshold:\", best_t)\n",
    "print(\"Precision:\", best_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72e318b5-547e-4dff-8afa-8d91854b2f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='274' max='137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [137/137 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL: {'eval_loss': 0.045941561460494995, 'eval_precision': 0.6607862903225806, 'eval_recall': 0.6008249312557287, 'eval_binary_accuracy': 0.988768131756216, 'eval_auc_roc_macro': 0.949046847466254, 'eval_auc_pr_macro': 0.6116958517505311, 'eval_runtime': 5.6738, 'eval_samples_per_second': 192.288, 'eval_steps_per_second': 24.146, 'epoch': 5.0}\n",
      "TEST: {'eval_loss': 0.04479491710662842, 'eval_precision': 0.6634370219275879, 'eval_recall': 0.5962419798350137, 'eval_binary_accuracy': 0.9887899553344099, 'eval_auc_roc_macro': 0.9528461849544706, 'eval_auc_pr_macro': 0.607380430076341, 'eval_runtime': 5.9159, 'eval_samples_per_second': 184.418, 'eval_steps_per_second': 23.158, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = float(best_t)\n",
    "\n",
    "print(\"VAL:\", trainer.evaluate(val_ds))\n",
    "print(\"TEST:\", trainer.evaluate(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9233a10-f3d1-4cb8-ab48-374984740eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008677716 0.017978659 0.107812636\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(val_ds)\n",
    "probs = 1/(1+np.exp(-pred.predictions))\n",
    "print(probs.min(), probs.mean(), probs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173efaa4-f714-4b80-b758-62561c51587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(val_ds)\n",
    "probs = 1/(1+np.exp(-pred.predictions))\n",
    "preds = (probs >= THRESHOLD).astype(int)\n",
    "\n",
    "print(\"true avg positives:\", val_y.sum(axis=1).mean())   # ~2.0\n",
    "print(\"pred avg positives:\", preds.sum(axis=1).mean())\n",
    "print(\"prob max:\", probs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266aff0c-c52b-494d-9f5b-9ce00f493e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5ee329-e9b9-4368-a42c-ea03d84e3970",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue avg positives:\u001b[39m\u001b[38;5;124m\"\u001b[39m, val_y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_y' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"true avg positives:\", val_y.sum(axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23528b70-a8f7-4b0a-9bd6-9dde70a32f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max prob: 0.107812636\n",
      "95 percentile: 0.06584469974040985\n"
     ]
    }
   ],
   "source": [
    "print(\"max prob:\", probs.max())\n",
    "print(\"95 percentile:\", np.percentile(probs, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b5249da-fd90-4bde-9e3d-8c1ddd94a917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true avg positives: 2.0\n",
      "threshold=0.02  pred avg positives=74.33\n",
      "threshold=0.03  pred avg positives=29.57\n",
      "threshold=0.04  pred avg positives=14.26\n",
      "threshold=0.05  pred avg positives=8.67\n",
      "threshold=0.06  pred avg positives=6.06\n",
      "threshold=0.07  pred avg positives=4.38\n",
      "threshold=0.08  pred avg positives=3.33\n",
      "threshold=0.09  pred avg positives=2.63\n",
      "threshold=0.10  pred avg positives=2.14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pred = trainer.predict(val_ds)\n",
    "probs = 1/(1+np.exp(-pred.predictions))\n",
    "\n",
    "true_avg = val_y.sum(axis=1).mean()\n",
    "print(\"true avg positives:\", true_avg)\n",
    "\n",
    "for t in [0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10]:\n",
    "    preds = (probs >= t).astype(int)\n",
    "    avg_pos = preds.sum(axis=1).mean()\n",
    "    print(f\"threshold={t:.2f}  pred avg positives={avg_pos:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6acf9-aa5f-4be9-ae5f-13af6dfe8b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb2bff5-c1ab-44a5-b82f-046ab349f5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb5014-3501-4d61-aa18-ae7e74e7dc93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
