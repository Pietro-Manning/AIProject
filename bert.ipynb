{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303eaf90-cfc3-4021-b0b3-e5acb44a9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def remove_character_from_column(\n",
    "    df: pd.DataFrame,\n",
    "    columname: str,\n",
    "    character_to_remove: str,\n",
    "    new_character: str = \"\",\n",
    "    is_regex: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace occurrences of pattern/character in a DataFrame column.\n",
    "    \"\"\"\n",
    "    if columname not in df.columns:\n",
    "        raise ValueError(f\"Column '{columname}' not found in DataFrame. Available: {list(df.columns)}\")\n",
    "\n",
    "    df = df.copy()\n",
    "   \n",
    "    s = df[columname].fillna(\"\").astype(str)\n",
    "\n",
    "    if is_regex:\n",
    "        df[columname] = s.str.replace(character_to_remove, new_character, regex=True)\n",
    "    else:\n",
    "        df[columname] = s.str.replace(character_to_remove, new_character, regex=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_characters(df_to_replace, columname='text', character_patterns=None, is_regex=True):\n",
    "    \"\"\"\n",
    "    La tua funzione, identica, ma in versione standalone.\n",
    "    \"\"\"\n",
    "    if character_patterns is None:\n",
    "        character_patterns = [\n",
    "            (r'[\\u2000-\\u200B\\u3000\\xa0]', ' '),       # Unicode spaces\n",
    "            (r'[\\u200C\\u200D\\u2060\\uFEFF]', ''),       # Invisible characters\n",
    "            (r'[\\u201C\\u201D\\u2018\\u2019]', '\"'),      # Quotation marks\n",
    "            (r'[\\u2022\\u2043]', ''),                   # List symbols\n",
    "            (r'[\\U0001F600-\\U0001F64F]', ''),          # Emoji\n",
    "            (r'[\\u2026]', '...'),                      # Ellipsis\n",
    "            (r\"http\\S+|www.\\S+\", \"\"),                  # URL\n",
    "            (r\"[\\n\\t\\r]\", \"\")                          # Tabulation and newline characters\n",
    "        ]\n",
    "    else:\n",
    "        if not isinstance(character_patterns, list):\n",
    "            raise TypeError(\"character_patterns must be a list of tuples\")\n",
    "\n",
    "    for pattern, replacement in character_patterns:\n",
    "        df_to_replace = remove_character_from_column(\n",
    "            df_to_replace,\n",
    "            columname=columname,\n",
    "            character_to_remove=pattern,\n",
    "            new_character=replacement,\n",
    "            is_regex=is_regex\n",
    "        )\n",
    "\n",
    "    return df_to_replace\n",
    "\n",
    "\n",
    "def write_classes_to_file(classes, out_dir: str, filename: str = \"classes.txt\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    path = os.path.join(out_dir, filename)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for c in classes:\n",
    "            f.write(f\"{c}\\n\")\n",
    "    return path\n",
    "\n",
    "def read_classes_from_file(out_dir: str, filename: str = \"classes.txt\"):\n",
    "    path = os.path.join(out_dir, filename)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "\n",
    "def get_and_preprocess_mnds(\n",
    "    dim: int | None = None,\n",
    "    cache_dir: str = \"./data_mnds\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Simile al tuo:\n",
    "      - usa caching su JSONL\n",
    "      - usa ML Binarizer su category_level_1 + category_level_2\n",
    "      - pulisce il testo con replace_characters (la tua)\n",
    "      - restituisce tf.data.Dataset((text, multi_hot)) + classes_names\n",
    "    \"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    suffix = f\"_{dim}\" if dim is not None else \"\"\n",
    "    dataset_path = os.path.join(cache_dir, f\"mnds{suffix}.jsonl\")\n",
    "    raw_csv_path = os.path.join(cache_dir, f\"mnds{suffix}.csv\")\n",
    "    classes_path = os.path.join(cache_dir, \"classes.txt\")\n",
    "\n",
    "\n",
    "    if os.path.exists(dataset_path) and os.path.exists(classes_path):\n",
    "        df_final = pd.read_json(dataset_path, orient=\"records\", lines=True)\n",
    "        df_final_text = df_final[\"text\"].astype(str).values\n",
    "        df_final[\"category\"] = df_final[\"category\"].apply(lambda x: np.array(x, dtype=np.int32))\n",
    "        df_final_categories = np.array(df_final[\"category\"].tolist(), dtype=np.int32)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((df_final_text, df_final_categories))\n",
    "        classes_names = read_classes_from_file(cache_dir)\n",
    "        return dataset, classes_names\n",
    "\n",
    "\n",
    "    split = f\"train[:{dim}]\" if dim is not None else \"train\"\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "\n",
    "    if os.path.exists(raw_csv_path):\n",
    "        df = pd.read_csv(raw_csv_path)\n",
    "    else:\n",
    "        ds = load_dataset(\"textminr/mn-ds\", split=split)\n",
    "        ds.to_csv(raw_csv_path, index=False)\n",
    "        df = pd.DataFrame(ds)\n",
    "\n",
    "\n",
    "    if \"content\" in df.columns and \"text\" not in df.columns:\n",
    "        df.rename(columns={\"content\": \"text\"}, inplace=True)\n",
    "\n",
    "\n",
    "    needed = [\"text\", \"category_level_1\", \"category_level_2\"]\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonne mancanti: {missing}. Disponibili: {list(df.columns)}\")\n",
    "\n",
    "    df = df[needed].copy()\n",
    "    df[\"category\"] = None  \n",
    "\n",
    "\n",
    "    df_final = replace_characters(df, columname=\"text\")\n",
    "\n",
    "\n",
    "    category = (df_final[[\"category\", \"category_level_1\", \"category_level_2\"]].fillna(\"\").values.tolist())\n",
    "    category = [[el for el in row if str(el).strip() != \"\"] for row in category]\n",
    "\n",
    "    category_encoded = mlb.fit_transform(category).astype(np.int32)\n",
    "\n",
    "    df_final[\"category\"] = list(category_encoded)\n",
    "    df_final = df_final[[\"text\", \"category\"]].copy()\n",
    "\n",
    "    df_final_text = df_final[\"text\"].astype(str).values\n",
    "    df_final_categories = np.array(df_final[\"category\"].tolist(), dtype=np.int32)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((df_final_text, df_final_categories))\n",
    "\n",
    "\n",
    "    write_classes_to_file(mlb.classes_, cache_dir, filename=\"classes.txt\")\n",
    "    df_final.to_json(dataset_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "    classes_names = list(mlb.classes_)\n",
    "    return dataset, classes_names\n",
    "\n",
    "\n",
    "# Split \n",
    "\n",
    "def split_dataset(dataset, test_size: float = 0.1, val_size: float = 0.1, random_state: int = 42):\n",
    "    if not isinstance(dataset, (pd.DataFrame, tf.data.Dataset)):\n",
    "        raise TypeError(\"dataset must be either a Pandas DataFrame or a TensorFlow dataset\")\n",
    "    if not isinstance(test_size, float) or not isinstance(val_size, float):\n",
    "        raise TypeError(\"test_size and val_size must be floats\")\n",
    "    if not isinstance(random_state, int):\n",
    "        raise TypeError(\"random_state must be an integer\")\n",
    "    if test_size < 0.0 or test_size > 1.0:\n",
    "        raise ValueError(\"test_size must be between 0.0 and 1.0\")\n",
    "    if val_size < 0.0 or val_size > 1.0:\n",
    "        raise ValueError(\"val_size must be between 0.0 and 1.0\")\n",
    "    if random_state < 0:\n",
    "        raise ValueError(\"random_state must be a positive integer\")\n",
    "    if test_size + val_size >= 1.0:\n",
    "        raise ValueError(\"test_size + val_size must be < 1.0\")\n",
    "\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        return split_df_dataset(dataset, random_state, test_size, val_size)\n",
    "    else:\n",
    "        return split_tf_dataset(dataset, test_size, val_size, random_state)\n",
    "\n",
    "\n",
    "def split_df_dataset(dataset, random_state, test_size, val_size):\n",
    "    df_train_val, df_test = train_test_split(\n",
    "        dataset, test_size=test_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    val_relative = val_size / (1.0 - test_size)\n",
    "    df_train, df_val = train_test_split(\n",
    "        df_train_val, test_size=val_relative, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "def split_tf_dataset(dataset: tf.data.Dataset, test_size=0.1, val_size=0.1, random_seed=42, buffer_size=10000):\n",
    "    dataset = dataset.shuffle(buffer_size=buffer_size, seed=random_seed, reshuffle_each_iteration=False)\n",
    "\n",
    "    dataset_size = int(dataset.reduce(0, lambda x, _: x + 1).numpy())\n",
    "\n",
    "    test_n = int(test_size * dataset_size)\n",
    "    val_n = int(val_size * dataset_size)\n",
    "    train_n = dataset_size - test_n - val_n\n",
    "\n",
    "    train_dataset = dataset.take(train_n)\n",
    "    rest_dataset = dataset.skip(train_n)\n",
    "\n",
    "    val_dataset = rest_dataset.take(val_n)\n",
    "    test_dataset = rest_dataset.skip(val_n)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset, classes = get_and_preprocess_mnds(dim=2000, cache_dir=\"./data_mnds\")\n",
    "    print(\"labels:\", len(classes), classes[:10])\n",
    "\n",
    "    train_ds, val_ds, test_ds = split_dataset(dataset, test_size=0.1, val_size=0.1)\n",
    "    print(train_ds, val_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216f652-8b42-4f45-bf39-f2957965dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "cache_dir = \"./data_mnds\"\n",
    "\n",
    "\n",
    "files = glob.glob(os.path.join(cache_dir, \"mnds*.jsonl\"))\n",
    "files += glob.glob(os.path.join(cache_dir, \"classes.txt\"))\n",
    "\n",
    "for f in files:\n",
    "    print(\"Elimino:\", f)\n",
    "    os.remove(f)\n",
    "\n",
    "print(\"Cache pulita.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061414b-0690-4bdb-9d11-db7037781af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544de4eb-ddb3-4c3b-af4c-e665f3fb44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(\"CUDA disponibile:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8970f6-dd94-490c-8693-5cc5f02e3b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667b1b8-00b5-4bd8-af1f-ddd60d93ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import sklearn\n",
    "\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d6465-e7d4-4f1b-9ecf-e0694d3b2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2743fa2-59c8-4bbf-b546-ba957887cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28cc2b2-efa8-4294-84a5-af5256a5f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, classes_names = get_and_preprocess_mnds(\n",
    "    dim=None,\n",
    "    cache_dir=\"./data_mnds\"\n",
    ")\n",
    "\n",
    "NUM_LABELS = len(classes_names)\n",
    "print(\"NUM_LABELS:\", NUM_LABELS)\n",
    "\n",
    "train_tf, val_tf, test_tf = split_dataset(\n",
    "    dataset,\n",
    "    test_size=0.1,\n",
    "    val_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "def tfds_to_numpy(ds, limit=None):\n",
    "    texts, labels = [], []\n",
    "    for i, (t, y) in enumerate(ds):\n",
    "        if limit is not None and i >= limit:\n",
    "            break\n",
    "        texts.append(t.numpy().decode(\"utf-8\"))\n",
    "        labels.append(y.numpy().astype(np.float32))\n",
    "    return texts, np.stack(labels)\n",
    "\n",
    "train_texts, train_y = tfds_to_numpy(train_tf)\n",
    "val_texts,   val_y   = tfds_to_numpy(val_tf)\n",
    "test_texts,  test_y  = tfds_to_numpy(test_tf)\n",
    "\n",
    "print(\"sizes:\", len(train_texts), len(val_texts), len(test_texts))\n",
    "print(\"label shape:\", train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41da6c-d02f-4b1c-8a38-93a71b50f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "pos = train_y.sum(axis=0)\n",
    "neg = train_y.shape[0] - pos\n",
    "\n",
    "pos_weight_base = (neg / (pos + 1e-8)).astype(np.float32)\n",
    "\n",
    "ALPHA = 0.5  \n",
    "pos_weight = ALPHA * pos_weight_base\n",
    "\n",
    "pos_weight_t = torch.tensor(pos_weight, device=device)\n",
    "\n",
    "print(\"pos_weight stats:\",\n",
    "      pos_weight.min(),\n",
    "      pos_weight.mean(),\n",
    "      pos_weight.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1b8a5-7e48-4ec2-80b8-3308f7bc071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import Trainer\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, pos_weight=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "    \n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d3ecc-0771-40f7-8826-89c5756d2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_all(texts, name):\n",
    "    print(f\"Tokenizing {name} ({len(texts)}) ...\")\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    print(f\"Done {name}\")\n",
    "    return enc\n",
    "\n",
    "train_enc = tokenize_all(train_texts, \"train\")\n",
    "val_enc   = tokenize_all(val_texts, \"val\")\n",
    "test_enc  = tokenize_all(test_texts, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4955c-e579-4190-a6d9-6486a0207699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodedMultiLabelDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            k: torch.tensor(v[idx], dtype=torch.long)\n",
    "            for k, v in self.encodings.items()\n",
    "        }\n",
    "        item[\"labels\"] = torch.tensor(\n",
    "            self.labels[idx], dtype=torch.float32\n",
    "        )\n",
    "        return item\n",
    "\n",
    "train_ds = EncodedMultiLabelDataset(train_enc, train_y)\n",
    "val_ds   = EncodedMultiLabelDataset(val_enc, val_y)\n",
    "test_ds  = EncodedMultiLabelDataset(test_enc, test_y)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202072b-3155-4368-b3cc-970885d8edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e03dc-b890-4638-8d9a-d67a6c08be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.105\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def macro_auc_per_label(y_true, y_score, kind=\"roc\"):\n",
    "    aucs = []\n",
    "    for j in range(y_true.shape[1]):\n",
    "        yt = y_true[:, j]\n",
    "        ys = y_score[:, j]\n",
    "        if np.unique(yt).size < 2:\n",
    "            continue\n",
    "        try:\n",
    "            if kind == \"roc\":\n",
    "                aucs.append(roc_auc_score(yt, ys))\n",
    "            else:\n",
    "                aucs.append(average_precision_score(yt, ys))\n",
    "        except Exception:\n",
    "            continue\n",
    "    return float(np.mean(aucs)) if aucs else float(\"nan\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = sigmoid(logits)\n",
    "\n",
    "    preds = (probs >= THRESHOLD).astype(int)\n",
    "    y_true = labels.astype(int)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(\n",
    "            y_true, preds, average=\"micro\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            y_true, preds, average=\"micro\", zero_division=0\n",
    "        ),\n",
    "        \"binary_accuracy\": accuracy_score(\n",
    "            y_true.reshape(-1),\n",
    "            preds.reshape(-1),\n",
    "        ),\n",
    "        \"auc_roc_macro\": macro_auc_per_label(\n",
    "            y_true, probs, kind=\"roc\"\n",
    "        ),\n",
    "        \"auc_pr_macro\": macro_auc_per_label(\n",
    "            y_true, probs, kind=\"pr\"\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bacb9a-2a19-458f-86c9-a7b51a4b31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./bert_mnds_pt\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=5,               \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"auc_pr_macro\",  \n",
    "    greater_is_better=True,\n",
    "    fp16=(device.type == \"cuda\"),\n",
    "    dataloader_pin_memory=(device.type == \"cuda\"),\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    pos_weight=pos_weight_t,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4821827-f1d8-420b-8401-93d538d61c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f7d48-d65b-48eb-b6dd-488d82f4e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "print(\"\\n--- TEST ---\")\n",
    "trainer.evaluate(test_ds)\n",
    "\n",
    "SAVE_DIR = \"./bert_mnds_pt/best_model\"\n",
    "trainer.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(f\"Model saved in {SAVE_DIR}\")\n",
    "\n",
    "\n",
    "def predict_multilabel(texts, top_k=10):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(\n",
    "            model(**enc).logits\n",
    "        ).cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for p in probs:\n",
    "        idx_sorted = np.argsort(-p)[:top_k]\n",
    "        picked = [\n",
    "            (classes_names[i], float(p[i]))\n",
    "            for i in idx_sorted\n",
    "            if p[i] >= THRESHOLD\n",
    "        ]\n",
    "        results.append(picked)\n",
    "    return results\n",
    "\n",
    "\n",
    "predict_multilabel([\n",
    "    \"Breaking news: a major earthquake caused damage and emergency response teams were deployed.\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195fd3b4-6760-4c00-b54e-eb45aeb9fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "pred = trainer.predict(val_ds)\n",
    "probs = 1/(1+np.exp(-pred.predictions))\n",
    "y_true = pred.label_ids.astype(int)\n",
    "\n",
    "best_t, best_p = None, 0.0\n",
    "\n",
    "for t in np.linspace(0.05, 0.5, 100):\n",
    "    y_pred = (probs >= t).astype(int)\n",
    "    p = precision_score(y_true.reshape(-1), y_pred.reshape(-1), zero_division=0)\n",
    "    r = recall_score(y_true.reshape(-1), y_pred.reshape(-1), zero_division=0)\n",
    "\n",
    "    if r >= 0.6 and p > best_p:\n",
    "        best_p = p\n",
    "        best_t = t\n",
    "\n",
    "print(\"Best threshold:\", best_t)\n",
    "print(\"Precision:\", best_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e318b5-547e-4dff-8afa-8d91854b2f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = float(best_t)\n",
    "\n",
    "print(\"VAL:\", trainer.evaluate(val_ds))\n",
    "print(\"TEST:\", trainer.evaluate(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9233a10-f3d1-4cb8-ab48-374984740eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(val_ds)\n",
    "probs = 1/(1+np.exp(-pred.predictions))\n",
    "print(probs.min(), probs.mean(), probs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173efaa4-f714-4b80-b758-62561c51587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(val_ds)\n",
    "probs = 1/(1+np.exp(-pred.predictions))\n",
    "preds = (probs >= THRESHOLD).astype(int)\n",
    "\n",
    "print(\"true avg positives:\", val_y.sum(axis=1).mean())   # ~2.0\n",
    "print(\"pred avg positives:\", preds.sum(axis=1).mean())\n",
    "print(\"prob max:\", probs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266aff0c-c52b-494d-9f5b-9ce00f493e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ee329-e9b9-4368-a42c-ea03d84e3970",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"true avg positives:\", val_y.sum(axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23528b70-a8f7-4b0a-9bd6-9dde70a32f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max prob:\", probs.max())\n",
    "print(\"95 percentile:\", np.percentile(probs, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5249da-fd90-4bde-9e3d-8c1ddd94a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pred = trainer.predict(val_ds)\n",
    "probs = 1/(1+np.exp(-pred.predictions))\n",
    "\n",
    "true_avg = val_y.sum(axis=1).mean()\n",
    "print(\"true avg positives:\", true_avg)\n",
    "\n",
    "for t in [0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10]:\n",
    "    preds = (probs >= t).astype(int)\n",
    "    avg_pos = preds.sum(axis=1).mean()\n",
    "    print(f\"threshold={t:.2f}  pred avg positives={avg_pos:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6acf9-aa5f-4be9-ae5f-13af6dfe8b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb2bff5-c1ab-44a5-b82f-046ab349f5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb5014-3501-4d61-aa18-ae7e74e7dc93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
